{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c6b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import plotly.io as pio\n",
    "import streamlit as st\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from utils.manager.login import *\n",
    "from utils.inputs.validation import *\n",
    "from utils.inputs.preprocess import *\n",
    "from utils.inputs.ads import *\n",
    "from utils.modeling.skforecast_utils import *\n",
    "from utils.modeling.sktime_utils import *\n",
    "\n",
    "# Set up the logging configuration for cmdstanpy\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Add NullHandler with CRITICAL log level\n",
    "null_handler = logging.NullHandler()\n",
    "null_handler.setLevel(logging.CRITICAL)\n",
    "logger.addHandler(null_handler)\n",
    "\n",
    "# Add StreamHandler with INFO log level\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ae5d9",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d814092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropdown for Country\n",
    "country_name = \"CA\"\n",
    "\n",
    "# Add dropdown for frequency\n",
    "forecast_freq = \"D\"\n",
    "\n",
    "# Add dropdown for data selection\n",
    "data_selection = False\n",
    "\n",
    "# Add dropdown for data selection\n",
    "external_features = False\n",
    "\n",
    "# Add file uploader to the sidebar\n",
    "uploaded_file = 'Agency Services.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397d1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if forecast_freq == \"D\":\n",
    "    forecast_period = 92\n",
    "elif forecast_freq == \"B\":\n",
    "    forecast_period = 66\n",
    "elif forecast_freq == \"W\":\n",
    "    forecast_period = 26\n",
    "elif forecast_freq == \"M\":\n",
    "    forecast_period = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea32277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize these selections into a dictionary\n",
    "run_params = {\n",
    "    \"country_name\": country_name,\n",
    "    \"forecast_freq\": forecast_freq,\n",
    "    \"forecast_period\": forecast_period,\n",
    "    \"data_selection\": data_selection,\n",
    "    \"external_features\": external_features,\n",
    "    \"weekend_weight\": 5,\n",
    "    \"holiday_weight\": 10,\n",
    "    \"metric_key\": \"mspe\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b710081",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb8549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Validate the input file\n",
    "    df = validate_input_file(uploaded_file, external_features)\n",
    "    logging.info(f\"Train Data Size: {df.shape}\")\n",
    "except Exception as e:\n",
    "    # Log this exception or handle it further up the call stack\n",
    "    raise Exception(f\"An error occurred while validating the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274398a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds      y\n",
       "0   2018-01-01    0.0\n",
       "1   2018-01-02  367.0\n",
       "2   2018-01-03  391.0\n",
       "3   2018-01-04  431.0\n",
       "4   2018-01-05  395.0\n",
       "..         ...    ...\n",
       "360 2018-12-27  280.0\n",
       "361 2018-12-28  278.0\n",
       "362 2018-12-29    0.0\n",
       "363 2018-12-30    0.0\n",
       "364 2018-12-31  299.0\n",
       "\n",
       "[365 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83fc477",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7e743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Process the input file\n",
    "    processed_df, forecast_df = process_input_file(df)\n",
    "    logging.info(f\"Train Data Size: {processed_df.shape}\")\n",
    "    logging.info(f\"Forecast Data Size: {forecast_df.shape}\")\n",
    "except Exception as e:\n",
    "    # Log this exception or handle it further up the call stack\n",
    "    raise Exception(f\"An error occurred while processing the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5daa6",
   "metadata": {},
   "source": [
    "## Automated Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44662db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if data_selection:\n",
    "        \n",
    "        # Find optimal window \n",
    "        optimal_window_size = find_optimal_window(processed_df)\n",
    "            \n",
    "        logging.info(f\"Optimal Window Size: {optimal_window_size}\")\n",
    "\n",
    "        # Add 180 days for feature engineering to optimal window\n",
    "        optimal_window_size += 180\n",
    "\n",
    "    else:\n",
    "        optimal_window_size = len(processed_df)\n",
    "        \n",
    "except Exception as e:\n",
    "    # Log this exception or handle it further up the call stack\n",
    "    raise Exception(f\"An error occurred while finding the optimal window: {str(e)}\")\n",
    "    \n",
    "\n",
    "run_params[\"optimal_window_size\"] = optimal_window_size\n",
    "    \n",
    "# Truncate the train set based on optimal window\n",
    "optimal_df = processed_df[-optimal_window_size:].copy(deep=True)\n",
    "    \n",
    "logging.info(f\"Optimal Train Data Size: {optimal_df.shape}\")\n",
    "\n",
    "# Find the min data for optimal train data\n",
    "run_params[\"optimal_window_date\"] = optimal_df['ds'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617a46d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99e458",
   "metadata": {},
   "source": [
    "## Final Data Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18642dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set forecast start and end dates\n",
    "min_forecast_date = optimal_df['ds'].min() + pd.Timedelta(days=1)\n",
    "max_forecast_date = min_forecast_date + pd.Timedelta(days=forecast_period)\n",
    "logging.info(f\"Forecast Range: {min_forecast_date} to {max_forecast_date}\")\n",
    "\n",
    "try:\n",
    "    # Validate column counts based on whether external features are used\n",
    "    if external_features:\n",
    "        assert optimal_df.shape[1] > 2 and forecast_df.shape[1] > 2\n",
    "    else:\n",
    "        assert optimal_df.shape[1] == 2 and forecast_df.shape[1] == 2\n",
    "    # Ensure non-empty data structure\n",
    "    assert optimal_df.shape[1] > 0\n",
    "    # Ensure same number of columns\n",
    "    assert optimal_df.shape[1] == forecast_df.shape[1]\n",
    "except Exception as e:\n",
    "    raise ValueError(\"Invalid input data format.\")\n",
    "\n",
    "try:\n",
    "    # Check coverage of forecast period by data\n",
    "    if external_features:\n",
    "        assert forecast_df['ds'].max() > max_forecast_date\n",
    "except Exception as e:\n",
    "    raise Exception(\"Incomplete external variable coverage for forecast period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7afc0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the exogenous variables from the train data\n",
    "run_params[\"exog_cols\"] = list((optimal_df.columns).difference(['y', 'ds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f94a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataframe(df, forecast_freq='D'):\n",
    "    \"\"\"\n",
    "    Resample and compute the mean for the dataframes based on a specified frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df.set_index('ds', inplace=True)\n",
    "    df = df.resample(forecast_freq).mean()\n",
    "    \n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a79f316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Generate date features\n",
    "    optimal_df = resample_dataframe(optimal_df, forecast_freq)\n",
    "    forecast_df = resample_dataframe(forecast_df, forecast_freq)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to set the data frequency to {forecast_freq}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "282a8449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import holidays  # Ensure the holidays library is installed and imported\n",
    "\n",
    "def generate_date_features(df: pd.DataFrame, freq='D', country_name=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add time-based features to a DataFrame based on its DateTime index, considering the frequency of data.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df.set_index('ds', inplace=True)\n",
    "    \n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        error_message = \"DataFrame must have a DateTimeIndex\"\n",
    "        logger.error(\"DataFrame must have a DateTimeIndex\")\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Suppress warnings during feature generation\n",
    "\n",
    "        # Generate features based on the specified frequency\n",
    "        if freq in ['D', 'B']:\n",
    "            # Features specific to daily data\n",
    "            df['day_of_week'] = df.index.dayofweek + 1  # Monday=1, Sunday=7\n",
    "            df['day_of_year'] = df.index.dayofyear\n",
    "            df['is_weekend'] = df.index.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "        if freq in ['D', 'B', 'W']:  # Weekly features include week_of_year\n",
    "            df['week_of_year'] = df.index.isocalendar().week.astype(int)\n",
    "\n",
    "        # Features applicable to all frequencies\n",
    "        df['quarter'] = df.index.quarter\n",
    "        df['month'] = df.index.month\n",
    "        df['year'] = df.index.year\n",
    "\n",
    "        # Calculate holidays if country_name is provided\n",
    "        if country_name:\n",
    "            country_holidays = holidays.CountryHoliday(country_name)\n",
    "            if freq in ['D', 'B']:\n",
    "                # Mark holidays for daily data\n",
    "                df['is_holiday'] = df.index.map(lambda date: int(date in country_holidays))\n",
    "            elif freq == 'W':\n",
    "                # Count holidays in a week for weekly data\n",
    "                df['is_holiday'] = df.index.map(lambda week_start: sum(\n",
    "                    1 for day in pd.date_range(start=week_start - pd.Timedelta(days=6), end=week_start)\n",
    "                    if day in country_holidays))\n",
    "            elif freq == 'M':\n",
    "                # Count holidays in a month for monthly data\n",
    "                df['is_holiday'] = df.index.map(lambda month_start: sum(\n",
    "                    1 for day in pd.date_range(start=month_start.replace(day=1), end=month_start)\n",
    "                    if day in country_holidays))\n",
    "\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf59268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Generate date features\n",
    "    optimal_df = generate_date_features(optimal_df, forecast_freq, country_name)\n",
    "    forecast_df = generate_date_features(forecast_df, forecast_freq, country_name)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to generate features using 'ds': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8632046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the exogenous variables from the train data\n",
    "run_params[\"exog_cols_all\"]  = list((optimal_df.columns).difference(['y', 'ds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd2bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_params(forecast_freq):\n",
    "    \"\"\"\n",
    "    Determines lag window and test step size based on the frequency and weekend inclusion.\n",
    "    \"\"\"\n",
    "    # Define settings for different scenarios using dictionaries\n",
    "    # initial_window_size, lag_window_range, rolling_window_range, test_size, test_steps\n",
    "    freq_settings = {\n",
    "        \"D\": (90, [7, 15, 30, 60, 90], [3, 7, 15, 30, 60, 90], 30, 3),\n",
    "        \"B\": (60, [5, 10, 20, 40, 60], [3, 5, 10, 20, 40, 60], 20, 2),\n",
    "        \"W\": (12, [4, 8, 12], [4, 8, 12, 16, 20, 24], 6, 1),\n",
    "        \"M\": (3, [3], [3, 6, 9, 12], 3, 1)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Select the appropriate settings based on forecast frequency and weekend drop\n",
    "        if forecast_freq in freq_settings:\n",
    "            if isinstance(freq_settings[forecast_freq], dict):\n",
    "                # Handle daily frequency differently based on weekend inclusion\n",
    "                return freq_settings[forecast_freq][weekend_drop]\n",
    "            else:\n",
    "                return freq_settings[forecast_freq]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown Frequency: {forecast_freq}\")\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to determine lag window and test set size: {e}\"\n",
    "        logger.error(error_message)\n",
    "        raise Exception(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f23cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    initial_window_size, lag_window_range, rolling_window_range, test_size, test_steps = determine_params(forecast_freq)\n",
    "    logger.info(f\"Initial Window Size: {initial_window_size}, Lag Window Range: {lag_window_range}\")\n",
    "    logger.info(f\"Test Size: {test_size}, Test Steps: {test_steps}\")\n",
    "except Exception as e:\n",
    "    raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5670ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params.update({\n",
    "    \"initial_window_size\": initial_window_size,\n",
    "    \"lag_window_range\": lag_window_range,\n",
    "    \"rolling_window_range\": rolling_window_range,\n",
    "    \"test_size\": test_size,\n",
    "    \"test_steps\": test_steps\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aacb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_df = optimal_df[-test_size:].copy(deep=True)\n",
    "    test_df = test_df.set_index('ds').asfreq(forecast_freq)\n",
    "    \n",
    "    train_df = optimal_df[:-test_size].copy(deep=True)\n",
    "    train_df = train_df.set_index('ds').asfreq(forecast_freq)\n",
    "    assert len(train_df) + len(test_df) == len(optimal_df)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to split into train and test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2e848a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'is_holiday' in train_df:\n",
    "    train_holiday_mask = train_df['is_holiday'].values\n",
    "if 'is_holiday' in test_df:\n",
    "    test_holiday_mask = test_df['is_holiday'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7adb7762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country_name': 'CA',\n",
       " 'forecast_freq': 'D',\n",
       " 'forecast_period': 92,\n",
       " 'data_selection': False,\n",
       " 'external_features': False,\n",
       " 'weekend_weight': 5,\n",
       " 'holiday_weight': 10,\n",
       " 'metric_key': 'mspe',\n",
       " 'optimal_window_size': 365,\n",
       " 'optimal_window_date': Timestamp('2018-01-01 00:00:00'),\n",
       " 'exog_cols': [],\n",
       " 'exog_cols_all': ['day_of_week',\n",
       "  'day_of_year',\n",
       "  'is_holiday',\n",
       "  'is_weekend',\n",
       "  'month',\n",
       "  'quarter',\n",
       "  'week_of_year',\n",
       "  'year'],\n",
       " 'initial_window_size': 90,\n",
       " 'lag_window_range': [7, 15, 30, 60, 90],\n",
       " 'rolling_window_range': [3, 7, 15, 30, 60, 90],\n",
       " 'test_size': 30,\n",
       " 'test_steps': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0073566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sktime.forecasting.fbprophet import Prophet\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "def load_model_params_and_create_instance(model_type, current_dir):\n",
    "    \"\"\"\n",
    "    Load model parameters from a YAML file and create a model instance based on model type.\n",
    "    \"\"\"\n",
    "    # Dictionary to map model types to their respective classes and YAML files\n",
    "    model_config = {\n",
    "        'random_forest': (RandomForestRegressor(), 'random_forest.yaml'),\n",
    "        'xgboost': (XGBRegressor(), 'xgboost.yaml'),\n",
    "        'prophet': (Prophet(), 'prophet.yaml'),\n",
    "        'naive': (NaiveForecaster(), 'naive.yaml')\n",
    "    }\n",
    "    \n",
    "    # Ensure the model type is supported\n",
    "    if model_type not in model_config:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    model_class, yaml_file = model_config[model_type]\n",
    "    file_path = os.path.join(current_dir, 'params', yaml_file)\n",
    "    \n",
    "    # Load parameters from the YAML file, handling file and parsing errors\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            param_grid = yaml.safe_load(file)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The configuration file {yaml_file} was not found in {file_path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise Exception(f\"Error parsing the YAML file: {e}\")\n",
    "\n",
    "    return model_class, param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96f0052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = 'utils/modeling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36d9dbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d506400d80426cb499db7690e40d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db43d849a37047848e40ae44773a8191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b5d7f6fd604d60a3c482fc2b8cdbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38967c4b62746a2a3abab47c8a71780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faebf732f7d04192a758e1a26680c1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47c3ac74bb24df9af8d1607320eb9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffa72a70d144e0ea0ff14ad42da78b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9388b77df2142bbb768131ea79662a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31121901fd394b5e9f849066c9b77146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dec14bff1f145f180022ba3b4fb3fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_results = {}\n",
    "\n",
    "for model_type in ['random_forest', 'xgboost']:\n",
    "    # Load parameters for grid search\n",
    "    model, param_grid = load_model_params_and_create_instance(model_type, current_dir)\n",
    "    \n",
    "    # Find best model\n",
    "    best_configuration, all_results, best_model = find_best_model_skforecast(\n",
    "        lag_window_range, model, train_df, param_grid, run_params\n",
    "    )\n",
    "    \n",
    "    # Save best model and config\n",
    "    search_results[model_type] = {\n",
    "        'best_model': best_model,\n",
    "        'best_configuration': best_configuration,\n",
    "        'all_results': all_results,\n",
    "        'package_type': 'skforecast'\n",
    "    }\n",
    "    \n",
    "    \n",
    "for model_type in ['prophet', 'naive']:\n",
    "    # Load parameters for grid search\n",
    "    model, param_grid = load_model_params_and_create_instance(model_type, current_dir)\n",
    "    \n",
    "    # Find best model\n",
    "    best_configuration, all_results, best_model = find_best_model_sktime(\n",
    "        train_df['y'], run_params, model, param_grid\n",
    "    )\n",
    "    \n",
    "    # Save best model and config\n",
    "    search_results[model_type] = {\n",
    "        'best_model': best_model,\n",
    "        'best_configuration': best_configuration,\n",
    "        'all_results': all_results,\n",
    "        'package_type': 'sktime'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa7825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebbf34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
