{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c6b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import plotly.io as pio\n",
    "import streamlit as st\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from utils.manager.login import *\n",
    "from utils.inputs.validation import *\n",
    "from utils.inputs.preprocess import *\n",
    "from utils.inputs.ads import *\n",
    "from utils.modeling.search import *\n",
    "from utils.modeling.general import *\n",
    "from utils.modeling.plot import *\n",
    "from utils.analysis.tables import *\n",
    "from utils.analysis.plot import *\n",
    "\n",
    "# Set up the logging configuration for cmdstanpy\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Add NullHandler with CRITICAL log level\n",
    "null_handler = logging.NullHandler()\n",
    "null_handler.setLevel(logging.CRITICAL)\n",
    "logger.addHandler(null_handler)\n",
    "\n",
    "# Add StreamHandler with INFO log level\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ae5d9",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d814092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropdown for Country\n",
    "country_name = \"CA\"\n",
    "\n",
    "# Add dropdown for frequency\n",
    "forecast_freq = \"D\"\n",
    "\n",
    "# Add dropdown for data selection\n",
    "data_selection = False\n",
    "\n",
    "# Add dropdown for data selection\n",
    "external_features = False\n",
    "\n",
    "# Add file uploader to the sidebar\n",
    "uploaded_file = 'Agency Services.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397d1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if forecast_freq == \"D\":\n",
    "    forecast_period = 92\n",
    "elif forecast_freq == \"B\":\n",
    "    forecast_period = 66\n",
    "elif forecast_freq == \"W\":\n",
    "    forecast_period = 26\n",
    "elif forecast_freq == \"M\":\n",
    "    forecast_period = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b710081",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb8549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Validate the input file\n",
    "    df = validate_input_file(uploaded_file, external_features)\n",
    "    logging.info(f\"Train Data Size: {df.shape}\")\n",
    "except Exception as e:\n",
    "    # Log this exception or handle it further up the call stack\n",
    "    raise Exception(f\"An error occurred while validating the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c7ffb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds      y\n",
       "0   2018-01-01    0.0\n",
       "1   2018-01-02  367.0\n",
       "2   2018-01-03  391.0\n",
       "3   2018-01-04  431.0\n",
       "4   2018-01-05  395.0\n",
       "..         ...    ...\n",
       "360 2018-12-27  280.0\n",
       "361 2018-12-28  278.0\n",
       "362 2018-12-29    0.0\n",
       "363 2018-12-30    0.0\n",
       "364 2018-12-31  299.0\n",
       "\n",
       "[365 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83fc477",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7e743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Process the input file\n",
    "    processed_df, forecast_df = process_input_file(df)\n",
    "    logging.info(f\"Train Data Size: {processed_df.shape}\")\n",
    "    logging.info(f\"Forecast Data Size: {forecast_df.shape}\")\n",
    "except Exception as e:\n",
    "    # Log this exception or handle it further up the call stack\n",
    "    raise Exception(f\"An error occurred while processing the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018a8a7",
   "metadata": {},
   "source": [
    "## Automated Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44662db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if data_selection:\n",
    "        \n",
    "        # Find optimal window \n",
    "        optimal_window_size = find_optimal_window(processed_df)\n",
    "            \n",
    "        logging.info(f\"Optimal Window Size: {optimal_window_size}\")\n",
    "\n",
    "        # Add 180 days for feature engineering to optimal window\n",
    "        optimal_window_size += 180\n",
    "\n",
    "    else:\n",
    "        optimal_window_size = len(processed_df)\n",
    "        \n",
    "except Exception as e:\n",
    "    # Log this exception or handle it further up the call stack\n",
    "    raise Exception(f\"An error occurred while finding the optimal window: {str(e)}\")\n",
    "    \n",
    "\n",
    "# Truncate the train set based on optimal window\n",
    "optimal_df = processed_df[-optimal_window_size:].copy(deep=True)\n",
    "    \n",
    "logging.info(f\"Optimal Train Data Size: {optimal_df.shape}\")\n",
    "\n",
    "# Find the min data for optimal train data\n",
    "optimal_window_date = optimal_df['ds'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9ff2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "646efaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds      y\n",
       "0   2018-01-01    0.0\n",
       "1   2018-01-02  367.0\n",
       "2   2018-01-03  391.0\n",
       "3   2018-01-04  431.0\n",
       "4   2018-01-05  395.0\n",
       "..         ...    ...\n",
       "360 2018-12-27  280.0\n",
       "361 2018-12-28  278.0\n",
       "362 2018-12-29    0.0\n",
       "363 2018-12-30    0.0\n",
       "364 2018-12-31  299.0\n",
       "\n",
       "[365 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a5813c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376395db",
   "metadata": {},
   "source": [
    "## Final Data Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5119b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set forecast start and end dates\n",
    "min_forecast_date = optimal_df['ds'].min() + pd.Timedelta(days=1)\n",
    "max_forecast_date = min_forecast_date + pd.Timedelta(days=forecast_period)\n",
    "logging.info(f\"Forecast Range: {min_forecast_date} to {max_forecast_date}\")\n",
    "\n",
    "try:\n",
    "    # Validate column counts based on whether external features are used\n",
    "    if external_features:\n",
    "        assert optimal_df.shape[1] > 2 and forecast_df.shape[1] > 2\n",
    "    else:\n",
    "        assert optimal_df.shape[1] == 2 and forecast_df.shape[1] == 2\n",
    "    # Ensure non-empty data structure\n",
    "    assert optimal_df.shape[1] > 0\n",
    "    # Ensure same number of columns\n",
    "    assert optimal_df.shape[1] == forecast_df.shape[1]\n",
    "except Exception as e:\n",
    "    raise ValueError(\"Invalid input data format.\")\n",
    "\n",
    "try:\n",
    "    # Check coverage of forecast period by data\n",
    "    if external_features:\n",
    "        assert forecast_df['ds'].max() > max_forecast_date\n",
    "except Exception as e:\n",
    "    raise Exception(\"Incomplete external variable coverage for forecast period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d5d579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the exogenous variables from the train data\n",
    "exog_cols = list((optimal_df.columns).difference(['y', 'ds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e1e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataframe(df, forecast_freq='D'):\n",
    "    \"\"\"\n",
    "    Resample and compute the mean for the dataframes based on a specified frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df.set_index('ds', inplace=True)\n",
    "    df = df.resample(forecast_freq).mean()\n",
    "    \n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac3eae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Generate date features\n",
    "    optimal_df = resample_dataframe(optimal_df, forecast_freq)\n",
    "    forecast_df = resample_dataframe(forecast_df, forecast_freq)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to set the data frequency to {forecast_freq}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a96361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import holidays  # Ensure the holidays library is installed and imported\n",
    "\n",
    "def generate_date_features(df: pd.DataFrame, freq='D', country_name=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add time-based features to a DataFrame based on its DateTime index, considering the frequency of data.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df.set_index('ds', inplace=True)\n",
    "    \n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        error_message = \"DataFrame must have a DateTimeIndex\"\n",
    "        logger.error(\"DataFrame must have a DateTimeIndex\")\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Suppress warnings during feature generation\n",
    "\n",
    "        # Generate features based on the specified frequency\n",
    "        if freq in ['D', 'B']:\n",
    "            # Features specific to daily data\n",
    "            df['day_of_week'] = df.index.dayofweek + 1  # Monday=1, Sunday=7\n",
    "            df['day_of_year'] = df.index.dayofyear\n",
    "            df['is_weekend'] = df.index.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "        if freq in ['D', 'B', 'W']:  # Weekly features include week_of_year\n",
    "            df['week_of_year'] = df.index.isocalendar().week.astype(int)\n",
    "\n",
    "        # Features applicable to all frequencies\n",
    "        df['quarter'] = df.index.quarter\n",
    "        df['month'] = df.index.month\n",
    "        df['year'] = df.index.year\n",
    "\n",
    "        # Calculate holidays if country_name is provided\n",
    "        if country_name:\n",
    "            country_holidays = holidays.CountryHoliday(country_name)\n",
    "            if freq in ['D', 'B']:\n",
    "                # Mark holidays for daily data\n",
    "                df['is_holiday'] = df.index.map(lambda date: int(date in country_holidays))\n",
    "            elif freq == 'W':\n",
    "                # Count holidays in a week for weekly data\n",
    "                df['is_holiday'] = df.index.map(lambda week_start: sum(\n",
    "                    1 for day in pd.date_range(start=week_start - pd.Timedelta(days=6), end=week_start)\n",
    "                    if day in country_holidays))\n",
    "            elif freq == 'M':\n",
    "                # Count holidays in a month for monthly data\n",
    "                df['is_holiday'] = df.index.map(lambda month_start: sum(\n",
    "                    1 for day in pd.date_range(start=month_start.replace(day=1), end=month_start)\n",
    "                    if day in country_holidays))\n",
    "\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3edfce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Generate date features\n",
    "    optimal_df = generate_date_features(optimal_df, forecast_freq, country_name)\n",
    "    forecast_df = generate_date_features(forecast_df, forecast_freq, country_name)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to generate features using 'ds': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56081a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the exogenous variables from the train data\n",
    "exog_cols_all = list((optimal_df.columns).difference(['y', 'ds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee253832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_params(forecast_freq):\n",
    "    \"\"\"\n",
    "    Determines lag window and test step size based on the frequency and weekend inclusion.\n",
    "    \"\"\"\n",
    "    # Define settings for different scenarios using dictionaries\n",
    "    # initial_window_size, lag_window_range, rolling_window_range, test_size, test_steps\n",
    "    freq_settings = {\n",
    "        \"D\": (90, [7, 15, 30, 60, 90], [3, 7, 15, 30, 60, 90], 30, 3),\n",
    "        \"B\": (60, [5, 10, 20, 40, 60], [3, 5, 10, 20, 40, 60], 20, 2),\n",
    "        \"W\": (12, [4, 8, 12], [4, 8, 12, 16, 20, 24], 6, 1),\n",
    "        \"M\": (3, [3], [3, 6, 9, 12], 3, 1)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Select the appropriate settings based on forecast frequency and weekend drop\n",
    "        if forecast_freq in freq_settings:\n",
    "            if isinstance(freq_settings[forecast_freq], dict):\n",
    "                # Handle daily frequency differently based on weekend inclusion\n",
    "                return freq_settings[forecast_freq][weekend_drop]\n",
    "            else:\n",
    "                return freq_settings[forecast_freq]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown Frequency: {forecast_freq}\")\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to determine lag window and test set size: {e}\"\n",
    "        logger.error(error_message)\n",
    "        raise Exception(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5f03e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    initial_window_size, lag_window_range, rolling_window_range, test_size, test_steps = determine_params(forecast_freq)\n",
    "    logger.info(f\"Initial Window Size: {initial_window_size}, Lag Window Range: {lag_window_range}\")\n",
    "    logger.info(f\"Test Size: {test_size}, Test Steps: {test_steps}\")\n",
    "except Exception as e:\n",
    "    raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd57d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_df = optimal_df[-test_size:].copy(deep=True)\n",
    "    train_df = optimal_df[:-test_size].copy(deep=True)\n",
    "    assert len(train_df) + len(test_df) == len(optimal_df)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to split into train and test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc20b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ds'] = pd.to_datetime(train_df['ds']) \n",
    "train_df.set_index('ds', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47867910",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'is_holiday' in train_df:\n",
    "    train_holiday_mask = train_df['is_holiday'].values\n",
    "if 'is_holiday' in test_df:\n",
    "    test_holiday_mask = test_df['is_holiday'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f627c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sktime.forecasting.fbprophet import Prophet\n",
    "from sktime.forecasting.naive import NaiveForecaster, NaiveVariance\n",
    "from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from typing import Optional, Tuple, List, Dict, Any, Callable, Union\n",
    "\n",
    "def load_model_params_and_create_instance(model_type, current_dir):\n",
    "    \"\"\"\n",
    "    Load model parameters from a YAML file and create a model instance based on model type.\n",
    "    \"\"\"\n",
    "    # Dictionary to map model types to their respective classes and YAML files\n",
    "    model_config = {\n",
    "        'random_forest': (RandomForestRegressor, 'random_forest.yaml'),\n",
    "        'xgboost': (XGBRegressor, 'xgboost.yaml'),\n",
    "        'prophet': (Prophet, 'prophet.yaml'),\n",
    "        'naive': (NaiveForecaster, 'naive.yaml')\n",
    "    }\n",
    "    \n",
    "    # Ensure the model type is supported\n",
    "    if model_type not in model_config:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    model_class, yaml_file = model_config[model_type]\n",
    "    file_path = os.path.join(current_dir, 'params', yaml_file)\n",
    "    \n",
    "    # Load parameters from the YAML file, handling file and parsing errors\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            param_grid = yaml.safe_load(file)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The configuration file {yaml_file} was not found in {file_path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise Exception(f\"Error parsing the YAML file: {e}\")\n",
    "\n",
    "    return model_class(), param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "300dfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_best_model_parameters_and_metrics(search_results: pd.DataFrame, metric_key: str) -> Tuple[Dict, float, int]:\n",
    "    \"\"\"\n",
    "    Get the parameters and metrics for the best model.\n",
    "    \"\"\"\n",
    "    best_params = search_results.iloc[0]['params']\n",
    "    best_score = search_results.iloc[0][metric_key]\n",
    "    lag_window = search_results.iloc[0]['lag_window']\n",
    "    return best_params, best_score, lag_window\n",
    "\n",
    "def _filter_holidays(y_true: pd.Series, y_pred: np.ndarray, country: str) -> Tuple[pd.Series, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Filters out holidays from the true and predicted series based on the specified country.\n",
    "    \"\"\"\n",
    "    country_holidays = holidays.CountryHoliday(country)\n",
    "    holiday_mask = y_true.index.map(lambda date: int(date in country_holidays))\n",
    "    holiday_mask = (holiday_mask == 1)\n",
    "\n",
    "    y_true = pd.Series([act for act, mask in zip(y_true, holiday_mask) if not mask])\n",
    "    y_pred = np.array([pred for pred, mask in zip(y_pred, holiday_mask) if not mask])\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "def _custom_mape(y_true: pd.Series, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Custom MAPE calculation considering country holidays.\n",
    "    \"\"\"\n",
    "    # Filter out holidays\n",
    "    if country:\n",
    "        y_true, y_pred = _filter_holidays(y_true, y_pred, country)\n",
    "\n",
    "    # Calculate errors\n",
    "    return mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "def _custom_mspe(y_true: pd.Series, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Custom MSPE calculation considering country holidays.\n",
    "    \"\"\"\n",
    "    # Filter out holidays\n",
    "    if country:\n",
    "        y_true, y_pred = _filter_holidays(y_true, y_pred, country)\n",
    "\n",
    "    # Calculate errors\n",
    "    return mean_squared_percentage_error(y_true, y_pred)\n",
    "\n",
    "def _custom_predictors(y: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to create custom predictors for a time series.\n",
    "    \"\"\"\n",
    "    predictors = []\n",
    "\n",
    "    # Calculate rolling statistics for specific rolling window sizes\n",
    "    for rolling_window in rolling_window_range:\n",
    "        predictors.extend([np.mean(y[-rolling_window:]), \n",
    "                           np.std(y[-rolling_window:]), \n",
    "                           np.min(y[-rolling_window:]), \n",
    "                           np.max(y[-rolling_window:])])\n",
    "\n",
    "    # Create lags\n",
    "    predictors.extend(y[-1:-lag_window-1:-1])\n",
    "\n",
    "    # Combine all predictors into one array\n",
    "    return np.hstack(predictors)\n",
    "\n",
    "def _custom_weights(index: pd.DatetimeIndex, country: str=None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return a list of weights for each index in the DataFrame.\n",
    "    \"\"\"\n",
    "    print(len(index))\n",
    "    print(index.values)\n",
    "    \n",
    "    \n",
    "    # Start with all weights as 1\n",
    "    weights = np.ones(len(index))\n",
    "    \n",
    "    print(index)\n",
    "\n",
    "    # Find indices of weekends\n",
    "    weekend_indices = index.dayofweek.isin([5, 6])\n",
    "\n",
    "    # Check for holidays if country is specified\n",
    "    if country:\n",
    "        country_holidays = holidays.CountryHoliday(country)\n",
    "        holiday_indices = index.map(lambda date: date in country_holidays)\n",
    "        weights[holiday_indices] = 10\n",
    "\n",
    "    # Set weights to 2 if either a weekend or a holiday\n",
    "    weights[weekend_indices] = 5\n",
    "\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfe35490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _create_forecaster(model: BaseEstimator, y: pd.Series, exog_cols: pd.DataFrame, \n",
    "                       param_grid: Dict, lag_window: int, test_steps: int, test_size: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a forecaster and perform a grid search to find the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "    forecaster = ForecasterAutoregCustom(regressor=model,\n",
    "                                         fun_predictors=_custom_predictors,\n",
    "                                         window_size=initial_window_size,\n",
    "                                         weight_func=_custom_weights)\n",
    "    \n",
    "    return grid_search_forecaster(forecaster=forecaster,\n",
    "                                  y=y,\n",
    "                                  exog=exog_cols,\n",
    "                                  param_grid=param_grid,\n",
    "                                  steps=test_steps,\n",
    "                                  fixed_train_size=False,\n",
    "                                  refit=True,\n",
    "                                  metric=[_custom_mape, _custom_mspe],\n",
    "                                  initial_train_size=len(y)-test_size,\n",
    "                                  return_best=False,\n",
    "                                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0c9fefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, [7, 15, 30, 60, 90], [3, 7, 15, 30, 60, 90], 30, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_window_size, lag_window_range, rolling_window_range, test_size, test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c82407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "256cf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    current_dir = 'utils/modeling'\n",
    "    model, param_grid = load_model_params_and_create_instance('random_forest', current_dir)\n",
    "    # model_xgb = load_model_params_and_create_instance('xgboost', current_dir)\n",
    "    # model_prophet = load_model_params_and_create_instance('prophet', current_dir)\n",
    "    # model_naive = load_model_params_and_create_instance('naive', current_dir)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error initializing models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18e05553",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['y'] = train_df['y'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06accd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_skforecast(lag_window_range: List[int], \n",
    "                           model: BaseEstimator, \n",
    "                           train_data: pd.DataFrame, \n",
    "                           exog_cols: List[str], \n",
    "                           param_grid: Dict) -> Tuple[pd.DataFrame, Dict, ForecasterAutoregCustom]:\n",
    "    \"\"\"\n",
    "    Function to perform grid search over different window sizes.\n",
    "    \"\"\"\n",
    "    window_size = 90\n",
    "    rolling_window_range = [7, 15, 30, 60, 90]\n",
    "    validation_steps = 15\n",
    "    validation_size = 60\n",
    "    \n",
    "    def _load_params() -> Dict:\n",
    "        \"\"\"\n",
    "        Load parameters from a yaml file.\n",
    "        \"\"\"\n",
    "        current_dir = 'utils/modeling'\n",
    "        file_path = os.path.join(current_dir, 'params', 'grid_search.yaml')\n",
    "        with open(file_path, 'r') as f:\n",
    "            return yaml.safe_load(f)\n",
    "\n",
    "    def _create_forecaster(model: BaseEstimator, y: pd.Series, exog_cols: pd.DataFrame, \n",
    "                           param_grid: Dict, lag_window: int, validation_steps: int, validation_size: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a forecaster and perform a grid search to find the best model.\n",
    "        \"\"\"\n",
    "        \n",
    "        forecaster = ForecasterAutoregCustom(regressor=model,\n",
    "                                             fun_predictors=_custom_predictors,\n",
    "                                             window_size=window_size,\n",
    "                                             weight_func=_custom_weights)\n",
    "\n",
    "        return grid_search_forecaster(forecaster=forecaster,\n",
    "                                      y=y,\n",
    "                                      exog=exog_cols,\n",
    "                                      param_grid=param_grid,\n",
    "                                      steps=validation_steps,\n",
    "                                      fixed_train_size=False,\n",
    "                                      refit=True,\n",
    "                                      metric=[_custom_mape, _custom_mspe],\n",
    "                                      initial_train_size=len(y)-validation_size,\n",
    "                                      return_best=False,\n",
    "                                      verbose=False)\n",
    "\n",
    "\n",
    "    def _get_best_model_parameters_and_metrics(search_results: pd.DataFrame, metric_key: str) -> Tuple[Dict, float, int]:\n",
    "        \"\"\"\n",
    "        Get the parameters and metrics for the best model.\n",
    "        \"\"\"\n",
    "        best_params = search_results.iloc[0]['params']\n",
    "        best_score = search_results.iloc[0][metric_key]\n",
    "        lag_window = search_results.iloc[0]['lag_window']\n",
    "        return best_params, best_score, lag_window\n",
    "    \n",
    "    def _filter_holidays(y_true: pd.Series, y_pred: np.ndarray, country: str) -> Tuple[pd.Series, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Filters out holidays from the true and predicted series based on the specified country.\n",
    "        \"\"\"\n",
    "        country_holidays = holidays.CountryHoliday(country)\n",
    "        holiday_mask = y_true.index.map(lambda date: int(date in country_holidays))\n",
    "        holiday_mask = (holiday_mask == 1)\n",
    "\n",
    "        y_true = pd.Series([act for act, mask in zip(y_true, holiday_mask) if not mask])\n",
    "        y_pred = np.array([pred for pred, mask in zip(y_pred, holiday_mask) if not mask])\n",
    "\n",
    "        return y_true, y_pred\n",
    "    \n",
    "    def _custom_mape(y_true: pd.Series, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Custom MAPE calculation considering country holidays.\n",
    "        \"\"\"\n",
    "        # Filter out holidays\n",
    "        if country:\n",
    "            y_true, y_pred = _filter_holidays(y_true, y_pred, country)\n",
    "\n",
    "        # Calculate errors\n",
    "        return mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "    def _custom_mspe(y_true: pd.Series, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Custom MSPE calculation considering country holidays.\n",
    "        \"\"\"\n",
    "        # Filter out holidays\n",
    "        if country:\n",
    "            y_true, y_pred = _filter_holidays(y_true, y_pred, country)\n",
    "\n",
    "        # Calculate errors\n",
    "        return mean_squared_percentage_error(y_true, y_pred)\n",
    "\n",
    "    def _custom_predictors(y: pd.Series) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function to create custom predictors for a time series.\n",
    "        \"\"\"\n",
    "        predictors = []\n",
    "\n",
    "        # Calculate rolling statistics for specific rolling window sizes\n",
    "        for rolling_window in rolling_window_range:\n",
    "            predictors.extend([np.mean(y[-rolling_window:]), \n",
    "                               np.std(y[-rolling_window:]), \n",
    "                               np.min(y[-rolling_window:]), \n",
    "                               np.max(y[-rolling_window:])])\n",
    "\n",
    "        # Create lags\n",
    "        predictors.extend(y[-1:-lag_window-1:-1])\n",
    "\n",
    "        # Combine all predictors into one array\n",
    "        return np.hstack(predictors)\n",
    "\n",
    "    def _custom_weights(index: pd.DatetimeIndex, country: str=None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return a list of weights for each index in the DataFrame.\n",
    "        \"\"\"\n",
    "        # Start with all weights as 1\n",
    "        weights = np.ones(len(index))\n",
    "\n",
    "        # Find indices of weekends\n",
    "        weekend_indices = index.dayofweek.isin([5, 6])\n",
    "\n",
    "        # Check for holidays if country is specified\n",
    "        if country:\n",
    "            country_holidays = holidays.CountryHoliday(country)\n",
    "            holiday_indices = index.map(lambda date: date in country_holidays)\n",
    "            weights[holiday_indices] = holiday_weight\n",
    "\n",
    "        # Set weights to 2 if either a weekend or a holiday\n",
    "        weights[weekend_indices] = weekend_weight\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    # Load parameters from yaml file\n",
    "    params = _load_params()\n",
    "    \n",
    "    metric_key = '_custom_mspe'\n",
    "    weekend_weight = params['weekend_weight']\n",
    "    holiday_weight = params['holiday_weight']\n",
    "\n",
    "    # Grid search over different window sizes\n",
    "    total_iterations = len(lag_window_range)\n",
    "    pbar = tqdm(total=total_iterations, desc=\"Grid Search Progress\")\n",
    "\n",
    "    # Perform grid search for each lag window and compile results\n",
    "    results_list = []\n",
    "    for lag_window in lag_window_range:\n",
    "        results_grid = _create_forecaster(model, train_data['y'], train_data[exog_cols], \n",
    "                                          param_grid, lag_window, validation_steps, validation_size)\n",
    "        results_grid['lag_window'] = lag_window\n",
    "        results_list.append(results_grid)\n",
    "        pbar.update()\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Get the best model parameters and metrics\n",
    "    search_results = pd.concat(results_list, ignore_index=True).sort_values(metric_key)\n",
    "    best_params, best_score, lag_window = _get_best_model_parameters_and_metrics(search_results, metric_key)\n",
    "    best_dict = {\"best_params\": best_params, \"best_score\": best_score, \"lag_window\": lag_window}\n",
    "\n",
    "    # Instantiate and train the best model\n",
    "    forecaster = ForecasterAutoregCustom(regressor=model.set_params(**best_params),\n",
    "                                         fun_predictors=_custom_predictors,\n",
    "                                         window_size=int(window_size))\n",
    "\n",
    "    return search_results, best_dict, forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ed52fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Grid Search Progress:   0%|                               | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models compared: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183f55d3b0c54e859e23cb75fbd885cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff983f37608455880dd7b996204f5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishekagarwal/opt/anaconda3/lib/python3.9/site-packages/skforecast/utils/utils.py:791: UserWarning: `y` has DatetimeIndex index but no frequency. Index is overwritten with a RangeIndex of step 1.\n",
      "  warnings.warn(\n",
      "/Users/abhishekagarwal/opt/anaconda3/lib/python3.9/site-packages/skforecast/utils/utils.py:917: UserWarning: `exog` has DatetimeIndex index but no frequency. Index is overwritten with a RangeIndex of step 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'dayofweek'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4t/nrkfp1595tb366616gpf9d540000gn/T/ipykernel_25883/2760355878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m grid_search_skforecast(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mlag_window_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog_cols_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     )\n",
      "\u001b[0;32m/var/folders/4t/nrkfp1595tb366616gpf9d540000gn/T/ipykernel_25883/2516613774.py\u001b[0m in \u001b[0;36mgrid_search_skforecast\u001b[0;34m(lag_window_range, model, train_data, exog_cols, param_grid)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mresults_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlag_window\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlag_window_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         results_grid = _create_forecaster(model, train_data['y'], train_data[exog_cols], \n\u001b[0m\u001b[1;32m    146\u001b[0m                                           param_grid, lag_window, validation_steps, validation_size)\n\u001b[1;32m    147\u001b[0m         \u001b[0mresults_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lag_window'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlag_window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4t/nrkfp1595tb366616gpf9d540000gn/T/ipykernel_25883/2516613774.py\u001b[0m in \u001b[0;36m_create_forecaster\u001b[0;34m(model, y, exog_cols, param_grid, lag_window, validation_steps, validation_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m                                              weight_func=_custom_weights)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         return grid_search_forecaster(forecaster=forecaster,\n\u001b[0m\u001b[1;32m     35\u001b[0m                                       \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                       \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skforecast/model_selection/model_selection.py\u001b[0m in \u001b[0;36mgrid_search_forecaster\u001b[0;34m(forecaster, y, param_grid, steps, metric, initial_train_size, fixed_train_size, gap, allow_incomplete_fold, exog, lags_grid, refit, return_best, verbose)\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m     results = _evaluate_grid_hyperparameters(\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0mforecaster\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0my\u001b[0m                     \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skforecast/model_selection/model_selection.py\u001b[0m in \u001b[0;36m_evaluate_grid_hyperparameters\u001b[0;34m(forecaster, y, param_grid, steps, metric, initial_train_size, fixed_train_size, gap, allow_incomplete_fold, exog, lags_grid, refit, return_best, verbose)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mforecaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m             metrics_values = backtesting_forecaster(\n\u001b[0m\u001b[1;32m   1272\u001b[0m                                  \u001b[0mforecaster\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                                  \u001b[0my\u001b[0m                     \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skforecast/model_selection/model_selection.py\u001b[0m in \u001b[0;36mbacktesting_forecaster\u001b[0;34m(forecaster, y, steps, metric, initial_train_size, fixed_train_size, gap, allow_incomplete_fold, exog, refit, interval, n_boot, random_state, in_sample_residuals, verbose, show_progress)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         metrics_values, backtest_predictions = _backtesting_forecaster_refit(\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0mforecaster\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mforecaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0my\u001b[0m                     \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skforecast/model_selection/model_selection.py\u001b[0m in \u001b[0;36m_backtesting_forecaster_refit\u001b[0;34m(forecaster, y, steps, metric, initial_train_size, fixed_train_size, gap, allow_incomplete_fold, exog, interval, n_boot, random_state, in_sample_residuals, verbose, show_progress)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mnext_window_exog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_idx_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         forecaster.fit(\n\u001b[0m\u001b[1;32m    479\u001b[0m             \u001b[0my\u001b[0m                         \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mexog\u001b[0m                      \u001b[0;34m=\u001b[0m \u001b[0mexog_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y, exog, store_in_sample_residuals)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_train_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_sample_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/skforecast/ForecasterAutoregCustom/ForecasterAutoregCustom.py\u001b[0m in \u001b[0;36mcreate_sample_weights\u001b[0;34m(self, X_train)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4t/nrkfp1595tb366616gpf9d540000gn/T/ipykernel_25883/2516613774.py\u001b[0m in \u001b[0;36m_custom_weights\u001b[0;34m(index, country)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Find indices of weekends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mweekend_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Check for holidays if country is specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'dayofweek'"
     ]
    }
   ],
   "source": [
    "grid_search_skforecast(\n",
    "        lag_window_range, model, train_df, exog_cols_all, param_grid\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30577998",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grid search over different window sizes\n",
    "total_iterations = len(lag_window_range)\n",
    "\n",
    "# Perform grid search for each lag window and compile results\n",
    "results_list = []\n",
    "for lag_window in lag_window_range:\n",
    "    results_grid = _create_forecaster(model, train_df['y'], train_df[exog_cols_all], \n",
    "                                      param_grid, lag_window, test_steps, test_size)\n",
    "    results_grid['lag_window'] = lag_window\n",
    "    results_list.append(results_grid)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def _get_best_model_parameters_and_metrics(search_results: pd.DataFrame, metric_key: str) -> Tuple[Dict, float, int]:\n",
    "    \"\"\"\n",
    "    Get the parameters and metrics for the best model.\n",
    "    \"\"\"\n",
    "    best_params = search_results.iloc[0]['params']\n",
    "    best_score = search_results.iloc[0][metric_key]\n",
    "    lag_window = search_results.iloc[0]['lag_window']\n",
    "    return best_params, best_score, lag_window\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load parameters from yaml file\n",
    "params = _load_params()\n",
    "\n",
    "metric_key = '_custom_mspe'\n",
    "weekend_weight = params['weekend_weight']\n",
    "holiday_weight = params['holiday_weight']\n",
    "\n",
    "# Determine the country based on session state\n",
    "country = _get_country()\n",
    "\n",
    "# Grid search over different window sizes\n",
    "total_iterations = len(lag_window_range)\n",
    "pbar = tqdm(total=total_iterations, desc=\"Grid Search Progress\")\n",
    "\n",
    "# Perform grid search for each lag window and compile results\n",
    "results_list = []\n",
    "for lag_window in lag_window_range:\n",
    "    results_grid = _create_forecaster(model, train_data['y'], train_data[exog_cols], \n",
    "                                      param_grid, lag_window, validation_steps, validation_size)\n",
    "    results_grid['lag_window'] = lag_window\n",
    "    results_list.append(results_grid)\n",
    "    pbar.update()\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Get the best model parameters and metrics\n",
    "search_results = pd.concat(results_list, ignore_index=True).sort_values(metric_key)\n",
    "best_params, best_score, lag_window = _get_best_model_parameters_and_metrics(search_results, metric_key)\n",
    "best_dict = {\"best_params\": best_params, \"best_score\": best_score, \"lag_window\": lag_window}\n",
    "\n",
    "# Instantiate and train the best model\n",
    "forecaster = ForecasterAutoregCustom(regressor=model.set_params(**best_params),\n",
    "                                     fun_predictors=_custom_predictors,\n",
    "                                     window_size=int(window_size))\n",
    "\n",
    "return search_results, best_dict, forecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from the xgboost.yaml file\n",
    "    file_path = os.path.join(current_dir, 'params', 'xgboost.yaml')\n",
    "    with open(file_path, 'r') as file:\n",
    "        param_grid_xgb = yaml.safe_load(file)\n",
    "\n",
    "    # Create an instance of the XGBoost regressor\n",
    "    model = XGBRegressor()\n",
    "\n",
    "    # Perform grid search on the XGBoost regressor\n",
    "    search_results_xgb, best_dict_xgb, forecaster_xgb = grid_search_skforecast(\n",
    "        lag_window_range, model, train_data, exog_cols, param_grid_xgb\n",
    "    )\n",
    "\n",
    "    logger.info(f\"XGBOOST - Best score: {best_dict_xgb['best_score']}, Best params: {best_dict_xgb['best_params']}\")\n",
    "\n",
    "    # Store the best estimator and model parameters in the session state\n",
    "    st.session_state.xgb_best_estimator = forecaster_xgb\n",
    "    st.session_state.xgb_best_model_params = best_dict_xgb\n",
    "\n",
    "    # Fit the best model on the train data and compute error metrics on the test data\n",
    "    best_model_xgb = forecaster_xgb\n",
    "    best_model_xgb.fit(y=train_data['y'], exog=train_data[exog_cols])\n",
    "\n",
    "    # Generate prediction intervals for the test data\n",
    "    predictions_xgb = forecaster_xgb.predict_interval(\n",
    "        steps=test_steps, exog=test_data[exog_cols], interval=[pi_lower, pi_upper], n_boot=pi_n_boots\n",
    "    )\n",
    "\n",
    "    # Compute error metrics and prediction coverage for the test data\n",
    "    st.session_state.xgb_test_metrics = compute_error_metrics(\n",
    "        actual, predictions_xgb['pred'], train_data['y'], holiday_mask\n",
    "    )\n",
    "    st.session_state.xgb_test_coverage = compute_prediction_coverage(test_data, predictions_xgb, holiday_mask)\n",
    "\n",
    "    # Fit the best model on the filtered train data\n",
    "    best_model_xgb = forecaster_xgb\n",
    "    best_model_xgb.fit(y=train_filtered['y'], exog=train_filtered[exog_cols])\n",
    "\n",
    "    # Store the best model in the session state\n",
    "    st.session_state.xgb_best_model = best_model_xgb\n",
    "    \n",
    "    # Store the feature importance of the best model\n",
    "    st.session_state.xgb_best_model_fi = best_model_xgb.get_feature_importances()\n",
    "    \n",
    "    # Update the progress bar with the time taken\n",
    "    end_time = time.time()\n",
    "    remaining_time = round(6*(end_time - start_time)/60, 2)\n",
    "    start_time = time.time()\n",
    "    st.session_state.search_time = round((end_time - init_time)/60, 2)\n",
    "\n",
    "    model_search_bar.progress(1/5, text=f\"⌛ 1/5 Completed! - Estimated Time Remaining: {remaining_time} minutes\")\n",
    "\n",
    "    #####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "y = np.array([100, 150, 200, 250, 300, 350, 400])\n",
    "rolling_window_range = [3, 5]  # Different window sizes for rolling calculations\n",
    "lag_window = 3  # Number of lagged values to include\n",
    "predictors = _custom_predictors(y)\n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a761c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
